{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import pickle\n",
    "import json\n",
    "from PIL import Image\n",
    "import clip\n",
    "from clip.model import AttentionPool2d\n",
    "from clip.model import ModifiedResNet\n",
    "from typing import Tuple, Union\n",
    "from clip.model import CLIP\n",
    "from clip.model import convert_weights\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "refcocog_path =  \"E:/DL_Datasets/refcocog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick = pickle.load(open(refcocog_path+\"/annotations/refs(umd).p\", \"rb\"))\n",
    "jsn = json.load(open(refcocog_path+\"/annotations/instances.json\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of all images\n",
    "images_set = {}\n",
    "for i in jsn['images']:\n",
    "  image_id = i['id']\n",
    "  images_set[image_id] = i\n",
    "\n",
    "# set of all annotations\n",
    "annotations_set = {}\n",
    "for a in jsn['annotations']:\n",
    "  annotation_id = a['id']\n",
    "  annotations_set[annotation_id] = a\n",
    "\n",
    "# set of all categories\n",
    "# categories_set = {}\n",
    "# for c in jsn['categories']:\n",
    "#   category_id = c['id']\n",
    "#   categories_set[category_id] = c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build dataset splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 80512, validation 4896, test9602\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label       = [], []\n",
    "validate_data, validate_label = [], []\n",
    "test_data, test_label         = [], []\n",
    "\n",
    "for p in pick:\n",
    "    data_image_path = f\"{refcocog_path}/images/{images_set[p['image_id']]['file_name']}\"\n",
    "    data_sentences = p['sentences']\n",
    "    data_bbox = annotations_set[p['ann_id']]['bbox']\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for s in data_sentences:\n",
    "        sentence = s['sent']\n",
    "        data.append([data_image_path, sentence, data_bbox])\n",
    "\n",
    "    if p['split'] == 'train':\n",
    "        train_data.extend(data)\n",
    "    elif p['split'] == 'test':\n",
    "        test_data.extend(data)\n",
    "    elif p['split'] == 'val':\n",
    "        validate_data.extend(data)\n",
    "\n",
    "print(f\"train {len(train_data)}, validation {len(validate_data)}, test {len(test_data)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display an image with a bounding box**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_image_with_bbox(image_path, prompt, bbox):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = np.asarray(image)\n",
    "\n",
    "    p1 = (int(bbox[0]), int(bbox[1]))\n",
    "    p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "\n",
    "    cv2.rectangle(image, p1, p2, (0,255,255), 3)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.title(prompt)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CLIP model\n",
    "clip_model, clip_preprocess = clip.load(\"RN50\")\n",
    "\n",
    "def linear(x, weight, bias):\n",
    "    x = x.matmul(weight.t())\n",
    "    x += bias\n",
    "    return x\n",
    "\n",
    "class AttentionSpatial2d(AttentionPool2d):\n",
    "    \"\"\"Edited attention pool layer to introduce spatial attention\"\"\"\n",
    "    def __init__(self,\n",
    "                 spacial_dim: int,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 output_dim: int = None):\n",
    "        super().__init__(spacial_dim, embed_dim, num_heads, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.shape\n",
    "        x = x.reshape(n, c, h*w).permute(2, 0, 1) # NCHW -> (H*W)NC\n",
    "        x = linear(x, self.v_proj.weight, self.v_proj.bias)\n",
    "        x = linear(x, self.c_proj.weight, self.c_proj.bias)\n",
    "        x = x.permute(1, 2, 0).reshape(n, -1, h, w) # (H*W)NC -> C(H*W)N -> (N, -1, H, W)\n",
    "        return x\n",
    "\n",
    "class ModifiedSpatialResNet(ModifiedResNet):\n",
    "    \"\"\"Modified resnet to include the edited attention pool layer\"\"\"\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 output_dim,\n",
    "                 heads,\n",
    "                 input_resolution=224,\n",
    "                 width=64):\n",
    "        super().__init__(layers, output_dim, heads, input_resolution, width)\n",
    "\n",
    "        self.attnpool = AttentionSpatial2d(\n",
    "            input_resolution // 32, width * 32, heads, output_dim)\n",
    "\n",
    "class CLIPSpatialResNet(CLIP):\n",
    "    \"\"\"Modified spatial CLIP including the spatial attention\"\"\"\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 image_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int):\n",
    "\n",
    "        super().__init__(embed_dim, image_resolution, vision_layers, vision_width,\n",
    "                         vision_patch_size, context_length, vocab_size,\n",
    "                         transformer_width, transformer_heads, transformer_layers)\n",
    "\n",
    "        # Override the visual model\n",
    "        vision_heads = vision_width * 32 // 64\n",
    "        self.visual = ModifiedSpatialResNet(layers=vision_layers,\n",
    "                                            output_dim=embed_dim,\n",
    "                                            heads=vision_heads,\n",
    "                                            input_resolution=image_resolution,\n",
    "                                            width=vision_width)\n",
    "\n",
    "    def forward(self, image):\n",
    "        image = image.type(self.dtype)\n",
    "\n",
    "        # pad image\n",
    "        pad = 64\n",
    "        pad = (pad, pad, pad, pad)\n",
    "        padded_image = F.pad(image, pad, \"constant\", 0)\n",
    "\n",
    "        # get features\n",
    "        features = self.encode_image(padded_image)\n",
    "        target_size_h, target_size_w = image.size(-2) // 32, image.size(-1) // 32\n",
    "\n",
    "        # compute new pad size\n",
    "        pad_h = (features.size(-2) - target_size_w) // 2\n",
    "        pad_w = (features.size(-1) - target_size_w) // 2\n",
    "        features = features[:, :, pad_h:pad_h+target_size_h, pad_w:pad_w+target_size_w]\n",
    "\n",
    "        # interpolate back to 224*224\n",
    "        features = F.upsample(features, size=(image.size(-2), image.size(-1)),\n",
    "            mode=\"bilinear\", align_corners=None) # 1*C*H*W\n",
    "\n",
    "        return features\n",
    "    \n",
    "\n",
    "def build_feature_extractor_model(clip_model): \n",
    "    \"\"\"\"Instantiate the modified CLIP model and adapt weights\"\"\"\n",
    "    # transfer learning: extract weights from CLIP\n",
    "    clip_state_dict = clip_model.state_dict()\n",
    "    # run [k for k in clip_state_dict if k.startswith(\"visual.layer2\")] to see what's up\n",
    "    counts: list = [len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n",
    "    vision_layers = tuple(counts)\n",
    "    vision_width = clip_state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
    "    output_width = round(\n",
    "        (clip_state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "\n",
    "    vision_patch_size = None\n",
    "    image_resolution = output_width * 32\n",
    "\n",
    "    embed_dim = clip_state_dict[\"text_projection\"].shape[1]\n",
    "    context_length = clip_state_dict[\"positional_embedding\"].shape[0]\n",
    "    vocab_size = clip_state_dict[\"token_embedding.weight\"].shape[0]\n",
    "    transformer_width = clip_state_dict[\"ln_final.weight\"].shape[0]\n",
    "    transformer_heads = transformer_width // 64\n",
    "    transformer_layers = len(set(\n",
    "        k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"transformer.resblocks\")))\n",
    "\n",
    "    model = CLIPSpatialResNet(\n",
    "        embed_dim,\n",
    "        image_resolution, vision_layers, vision_width, vision_patch_size,\n",
    "        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers).to(device)\n",
    "\n",
    "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
    "        if key in clip_state_dict:\n",
    "            del clip_state_dict[key]\n",
    "\n",
    "    convert_weights(model)\n",
    "\n",
    "    # False for the average filter layer.\n",
    "    model.load_state_dict(clip_state_dict, strict=False)\n",
    "    # model.eval()\n",
    "    if device == 'cpu':\n",
    "        model.float()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBox extraction\n",
    "\n",
    "This code is borrowed from the paper. It includes three models that can be used to extract the bounding box from the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BruteForceBoxSearch():\n",
    "    \"\"\"Use pytorch to speed up. If matrix is too large, it may be out of memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, downsample=8):\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def __call__(self, matrix, objective_cls):\n",
    "        h, w = matrix.shape[:2]\n",
    "        matrix = torch.from_numpy(matrix).to(device)\n",
    "        # get new size\n",
    "        self.h = h // self.downsample\n",
    "        self.w = w // self.downsample\n",
    "        # downsample matrix\n",
    "        self.matrix = F.interpolate(\n",
    "            matrix[None, None], (self.h, self.w), mode='bilinear')[0, 0]\n",
    "        # get objective object\n",
    "        self.objective = objective_cls(self.matrix)\n",
    "        # get full intervals\n",
    "        intervals = [[0, self.w-1], [0, self.h-1],\n",
    "                     [0, self.w-1], [0, self.h-1]]\n",
    "        # get coarse guess\n",
    "        anchor_box = self.search(intervals)\n",
    "        # rescale to original resolution\n",
    "        anchor_box *= self.downsample\n",
    "        x1, y1, x2, y2 = anchor_box\n",
    "        # offset of adjustment\n",
    "        offset_w = offset_h = self.downsample\n",
    "        # back to original matrix\n",
    "        self.matrix = matrix\n",
    "        self.h, self.w = h, w\n",
    "        # get new objective\n",
    "        self.objective = objective_cls(self.matrix)\n",
    "        # set intervals\n",
    "        intervals = [\n",
    "            [max(0, x1-offset_w), min(x1+offset_w, self.w-1)],\n",
    "            [max(0, y1-offset_h), min(y1+offset_h, self.h-1)],\n",
    "            [max(0, x2-offset_w), min(x2+offset_w, self.w-1)],\n",
    "            [max(0, y2-offset_h), min(y2+offset_h, self.h-1)],\n",
    "        ]\n",
    "        # search box\n",
    "        box = self.search(intervals)\n",
    "        return box\n",
    "\n",
    "    def search(self, intervals):\n",
    "        # intervals is like [[x1_min, x1_max], [y1_min, y1_max], ...]\n",
    "        x1 = torch.arange(intervals[0][0], intervals[0][1]+1).to(device)\n",
    "        y1 = torch.arange(intervals[1][0], intervals[1][1]+1).to(device)\n",
    "        x2 = torch.arange(intervals[2][0], intervals[2][1]+1).to(device)\n",
    "        y2 = torch.arange(intervals[3][0], intervals[3][1]+1).to(device)\n",
    "        boxes = torch.cartesian_prod(x1, y1, x2, y2)\n",
    "        x1, y1, x2, y2 = boxes.transpose(0, 1)\n",
    "        boxes = boxes[(x1 >= 0) & (y1 >= 0) & (x2 < self.w) &\n",
    "                      (y2 < self.h) & (x2 > x1) & (y2 > y1)]\n",
    "        box = boxes[self.objective.eval(boxes).argmax()]\n",
    "        box = box.cpu().numpy()\n",
    "        return box\n",
    "\n",
    "\n",
    "class SumAreaObjective():\n",
    "    \"\"\"f(x) = (sum inside x) - alpha * (normalized area of x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, matrix):\n",
    "        # precompute things\n",
    "        self.matrix = matrix\n",
    "        self.h, self.w = matrix.size(0), matrix.size(1)\n",
    "        # for normalizing area\n",
    "        self.area = float(self.h * self.w)\n",
    "        # assume matrix is all positive\n",
    "        self.sums = self.matrix.cumsum(1).cumsum(0)\n",
    "        # pad the sums on top and left to deal with boundary cases\n",
    "        self.sums = F.pad(self.sums, (1, 0, 1, 0))\n",
    "        # get total sum for computing fraction\n",
    "        self.total_sum = self.matrix.sum().item()\n",
    "        return self\n",
    "\n",
    "    def eval(self, boxes):\n",
    "        frac = self._compute_frac(boxes)\n",
    "        area = self._compute_area(boxes)\n",
    "        return frac - self.alpha * area\n",
    "\n",
    "    def _compute_frac(self, boxes):\n",
    "        # boxes is Nx4, each is [x1, y1, x2, y2]\n",
    "        x1, y1, x2, y2 = boxes.transpose(0, 1)\n",
    "        # assume all boxes are valid\n",
    "        bottom_right = self.sums[y2+1, x2+1]\n",
    "        bottom_left = self.sums[y2+1, x1]\n",
    "        top_right = self.sums[y1, x2+1]\n",
    "        top_left = self.sums[y1, x1]\n",
    "        box_sum = bottom_right - bottom_left - top_right + top_left\n",
    "        return box_sum / (self.total_sum + 1e-8)\n",
    "\n",
    "    def _compute_area(self, boxes):\n",
    "        # boxes is Nx4, each is [x1, y1, x2, y2]\n",
    "        x1, y1, x2, y2 = boxes.transpose(0, 1)\n",
    "        # assume all boxes are valid\n",
    "        return (x2 - x1 + 1).float() * (y2 - y1 + 1).float() / self.area\n",
    "\n",
    "\n",
    "class FractionAreaObjective():\n",
    "    \"\"\"f(x) = (fraction of sum inside x) - alpha * (normalized area of x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, matrix):\n",
    "        # precompute things\n",
    "        self.matrix = matrix\n",
    "        self.h, self.w = matrix.size(0), matrix.size(1)\n",
    "        # for normalizing area\n",
    "        self.area = float(self.h * self.w)\n",
    "        # assume matrix is all positive\n",
    "        self.sums = self.matrix.cumsum(1).cumsum(0)\n",
    "        # pad the sums on top and left to deal with boundary cases\n",
    "        self.sums = F.pad(self.sums, (1, 0, 1, 0))\n",
    "        # get total sum for computing fraction\n",
    "        self.total_sum = self.matrix.sum().item()\n",
    "        return self\n",
    "\n",
    "    def eval(self, boxes):\n",
    "        frac = self._compute_frac(boxes)\n",
    "        area = self._compute_area(boxes)\n",
    "        return frac - self.alpha * area\n",
    "\n",
    "    def _compute_frac(self, boxes):\n",
    "        # boxes is Nx4, each is [x1, y1, x2, y2]\n",
    "        x1, y1, x2, y2 = boxes.transpose(0, 1)\n",
    "        # assume all boxes are valid\n",
    "        bottom_right = self.sums[y2+1, x2+1]\n",
    "        bottom_left = self.sums[y2+1, x1]\n",
    "        top_right = self.sums[y1, x2+1]\n",
    "        top_left = self.sums[y1, x1]\n",
    "        box_sum = bottom_right - bottom_left - top_right + top_left\n",
    "        return box_sum / (self.total_sum + 1e-8)\n",
    "\n",
    "    def _compute_area(self, boxes):\n",
    "        # boxes is Nx4, each is [x1, y1, x2, y2]\n",
    "        x1, y1, x2, y2 = boxes.transpose(0, 1)\n",
    "        # assume all boxes are valid\n",
    "        return (x2 - x1 + 1).float() * (y2 - y1 + 1).float() / self.area"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetHighResV2(nn.Module):\n",
    "    \"\"\"Feature extractor that includes CLIP as its fundation model\"\"\"\n",
    "    def __init__(self, clip_preprocess, tokenize, temperature=0.1, alpha=0.7, remap_heatmaps=True):\n",
    "        super().__init__()\n",
    "        self.spatial_model = build_feature_extractor_model(clip_model)\n",
    "        self.clip_preprocess = clip_preprocess\n",
    "        self.tokenize = tokenize\n",
    "        self.temperature = temperature\n",
    "        self.remap_heatmaps=remap_heatmaps\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def get_image_features(self, images):\n",
    "        images = [clip_preprocess(image) for image in images]\n",
    "        images = torch.stack(images).to(device)\n",
    "        image_features = self.spatial_model(images)\n",
    "        return image_features\n",
    "\n",
    "    def get_text_features(self, texts):\n",
    "        tokenized_texts = self.tokenize(texts).to(device)\n",
    "        text_features = self.spatial_model.encode_text(tokenized_texts)\n",
    "        return text_features\n",
    "    \n",
    "    def box_from_heatmap(self, heatmap):\n",
    "        alpha = self.alpha\n",
    "        sum_map = heatmap.copy()\n",
    "        sum_map /= sum_map.sum() + 1e-8\n",
    "        sum_map -= alpha / sum_map.shape[0] / sum_map.shape[1]\n",
    "        bf = BruteForceBoxSearch()\n",
    "        objective = FractionAreaObjective(alpha=alpha)\n",
    "        box = bf(heatmap, objective)\n",
    "        box = box.astype(np.float32)[None]\n",
    "        return box\n",
    "    \n",
    "    def get_heatmaps(self, image_features, text_features):\n",
    "        image_features /= image_features.norm(dim=1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=1, keepdim=True)\n",
    "        heatmaps = (image_features * text_features[:, :, None, None]).sum(1)\n",
    "        heatmaps = torch.exp(heatmaps/0.025)\n",
    "        if self.remap_heatmaps:\n",
    "            for i in range(len(heatmaps)):          \n",
    "                min_ele = torch.min(heatmaps[i])\n",
    "                heatmaps[i] -= min_ele\n",
    "                heatmaps[i] /= torch.max(heatmaps[i])\n",
    "        return heatmaps\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        image_features = self.get_image_features(images)\n",
    "        text_features = self.get_text_features(texts)\n",
    "        heatmaps = self.get_heatmaps(image_features, text_features)\n",
    "        heatmaps = heatmaps.cpu().detach().float()\n",
    "        return heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatmapToBox(nn.Module):\n",
    "    \"\"\"Custom model to regress a bounding box from an heatmap\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()  \n",
    "        self.conv = nn.Sequential(                            \n",
    "            nn.Conv2d(1,1,8,stride=1), \n",
    "            nn.Conv2d(1,1,6,stride=2), \n",
    "            nn.Conv2d(1,1,4,stride=2)\n",
    "            )\n",
    "        self.seq = nn.Sequential(    \n",
    "            nn.Flatten(),\n",
    "            nn.AvgPool1d(4),\n",
    "\n",
    "            nn.Linear(676, 256),\n",
    "            nn.Dropout(p=0.05),            \n",
    "            nn.Sigmoid(),            \n",
    "\n",
    "            nn.Linear(256, 128),  \n",
    "            #nn.Dropout(p=0.05),          \n",
    "            nn.Sigmoid(),\n",
    "\n",
    "            nn.Linear(128, 4),\n",
    "            nn.Sigmoid(),          \n",
    "        )       \n",
    "        self.conv.apply(self.weights_init)\n",
    "        self.seq.apply(self.weights_init)\n",
    "        \n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(m.weight, 0.0,  .75)\n",
    "        if isinstance(m, torch.nn.BatchNorm1d):\n",
    "            torch.nn.init.normal_(m.weight, 0.0, .75)\n",
    "        if isinstance(m, torch.nn.Conv2d):\n",
    "            torch.nn.init.normal_(m.weight, 0.0, .75)\n",
    "            \n",
    "    def forward(self, x):   \n",
    "        return self.seq(self.conv(x.unsqueeze(dim=1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intersection Over Union**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import generalized_box_iou_loss\n",
    "from torchvision.ops.boxes import box_convert\n",
    "\n",
    "def iou(boxes1, boxes2) -> torch.Tensor:\n",
    "    return generalized_box_iou_loss(box_convert(boxes1,in_fmt=\"xywh\",out_fmt=\"xyxy\"),box_convert(boxes2,in_fmt=\"xywh\",out_fmt=\"xyxy\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 4096\n",
    "train_batch_size = 64\n",
    "epochs = 512\n",
    "mini_train_data = train_data[:train_size]\n",
    "\n",
    "validation_size = 1024\n",
    "validation_batch_size = 32\n",
    "validation_module = 6\n",
    "mini_val_data = validate_data[:validation_size]\n",
    "\n",
    "test_size = 512\n",
    "test_batch_size = 32\n",
    "mini_test_data = test_data[:test_size]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routines\n",
    "- Train\n",
    "- Validation\n",
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_routine(model, loss_fn, feature_extractor, optimizer):\n",
    "    model.train()\n",
    "    epoch_loss =[]\n",
    "    for i in range(0, train_size, train_batch_size):\n",
    "        batch_data = mini_train_data[i:i+train_batch_size]\n",
    "\n",
    "        images, target_boxes, prompts = [], [], []\n",
    "        for image_path, prompt, box in batch_data:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            w, h = image.size\n",
    "            correct_box = [\n",
    "                box[0] / w,\n",
    "                box[1] / h,\n",
    "                (box[0] + box[2]) / w,\n",
    "                (box[1] + box[3]) / h\n",
    "            ]\n",
    "            target_boxes.append(correct_box)\n",
    "            images.append(image)            \n",
    "            prompts.append(prompt)            \n",
    "\n",
    "        target_boxes = torch.tensor(target_boxes)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            heatmaps = feature_extractor(images, prompts)            \n",
    "        heatmaps_tensor = torch.tensor(np.array(heatmaps))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        prediction_boxes = model(heatmaps_tensor)\n",
    "        loss = loss_fn(prediction_boxes, target_boxes)        \n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return sum(epoch_loss) / len(epoch_loss)\n",
    "\n",
    "def validation_routine(model, loss_fn, feature_extractor):\n",
    "    model.eval()\n",
    "    epoch_loss = []\n",
    "    giou = []\n",
    "    print(\"running validation\")\n",
    "    for i in range(0, validation_size, validation_batch_size):\n",
    "        batch_data = mini_val_data[i:i+validation_batch_size]\n",
    "        images, target_boxes, prompts = [], [], []\n",
    "        for image_path, prompt, box in batch_data:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            w, h = image.size\n",
    "\n",
    "            correct_box = [\n",
    "                box[0] / w,\n",
    "                box[1] / h,\n",
    "                (box[0] + box[2]) / w,\n",
    "                (box[1] + box[3]) / h\n",
    "            ]\n",
    "            target_boxes.append(correct_box)\n",
    "            images.append(image)            \n",
    "            prompts.append(prompt)            \n",
    "\n",
    "        target_boxes = torch.tensor(target_boxes)\n",
    "        with torch.no_grad():\n",
    "            heatmaps = feature_extractor(images, prompts)\n",
    "            heatmaps_tensor = torch.tensor(np.array(heatmaps))\n",
    "            prediction_boxes = model(heatmaps_tensor)\n",
    "\n",
    "        loss = loss_fn(prediction_boxes, target_boxes)\n",
    "        epoch_loss.append(loss.item())\n",
    "        giou.append(iou(prediction_boxes, target_boxes))\n",
    "    return sum(epoch_loss) / len(epoch_loss), sum(giou) / len(giou)\n",
    "\n",
    "def test_routine(model, loss_fn, feature_extractor):\n",
    "    model.eval()\n",
    "    epoch_loss = []\n",
    "    giou = []\n",
    "    print(\"running test\")\n",
    "    for i in range(0, test_size, test_batch_size):\n",
    "        batch_data = mini_test_data[i:i+test_batch_size]\n",
    "        images, target_boxes, prompts = [], [], []\n",
    "        for image_path, prompt, box in batch_data:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            w, h = image.size\n",
    "\n",
    "            correct_box = [\n",
    "                box[0] / w,\n",
    "                box[1] / h,\n",
    "                (box[0] + box[2]) / w,\n",
    "                (box[1] + box[3]) / h\n",
    "            ]\n",
    "            target_boxes.append(correct_box)\n",
    "            images.append(image)            \n",
    "            prompts.append(prompt)            \n",
    "\n",
    "        target_boxes = torch.tensor(target_boxes)\n",
    "        with torch.no_grad():\n",
    "            heatmaps = feature_extractor(images, prompts)\n",
    "            heatmaps_tensor = torch.tensor(np.array(heatmaps))\n",
    "            prediction_boxes = model(heatmaps_tensor)\n",
    "\n",
    "        loss = loss_fn(prediction_boxes, target_boxes)\n",
    "        epoch_loss.append(loss.item())\n",
    "        giou.append(iou(prediction_boxes, target_boxes))\n",
    "    return sum(epoch_loss) / len(epoch_loss), sum(giou) / len(giou)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ResNetHighResV2(clip_preprocess, clip.tokenize, remap_heatmaps=False)\n",
    "loss_fn = nn.MSELoss()\n",
    "bboxer = HeatmapToBox()\n",
    "optimizer = torch.optim.Adam(params=bboxer.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = 1E3\n",
    "for epoch in range(epochs):\n",
    "    loss = training_routine(bboxer,loss_fn ,feature_extractor, optimizer)\n",
    "    print(f\"epoch {epoch}\")\n",
    "    print(f\"training_loss: {loss}\")\n",
    "    if epoch != 0 and epoch % validation_module == 0:        \n",
    "        val_loss, giou = validation_routine(bboxer, loss_fn, feature_extractor)\n",
    "        if val_loss+giou < best:\n",
    "            torch.save(bboxer.state_dict(), \"checkpoint\")\n",
    "            print(\"saving checkpoint\")\n",
    "            best=val_loss+giou        \n",
    "        print(f\"validation loss: {val_loss}, giou: {giou}\")                \n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "torch.save(bboxer.state_dict(), \"checkpoint\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_giou = test_routine(bboxer, loss_fn, feature_extractor)\n",
    "print(f\"test loss: {val_loss}, test giou: {giou}\")         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
