{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DL Project assignment\n",
        "\n",
        "**Members**  \n",
        "\n",
        "- _Matteo Beltrame_ `231443`    \n",
        "- _Shandy Darma_ `238925`     \n",
        "- _Zhu Fengyi_ `229748`    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XSXSEIG7QNKu"
      },
      "source": [
        "# Imports\n",
        "\n",
        "Remember to run this!\n",
        "\n",
        "This is required to import the required packages!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sNkTLvNNTXvO"
      },
      "source": [
        "### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf-XFykgTb2G",
        "outputId": "ca438ddf-834f-4bf4-fc1c-765d5bec12b4"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIiwTHMYQNKw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import csv\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import clip\n",
        "from clip.model import AttentionPool2d\n",
        "from clip.model import ModifiedResNet\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Tuple, Union\n",
        "from clip.model import CLIP\n",
        "from torchvision.ops import generalized_box_iou_loss\n",
        "from torchvision.ops.boxes import box_convert\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cwED9afFQNKx"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "In this section, the **RefCOCOg** dataset is loaded and the tree splits are generated:\n",
        "- `training_data`: split used to train the model, length: **80512**\n",
        "- `validation_data`: split used to validate the model during training, length: **4896**\n",
        "- `test_data`: split used to test the model after training, length: **9602**\n",
        "\n",
        "Total: **95010**\n",
        "\n",
        "The splits include exactly all samples from the dataset. This generates a very big training dataset that, due to computational limits, cannot be effectively exploited to fine tune the model to perfection.\n",
        "Notice that the images in the dataset are around **25000**. The dataset size however is this big since each prompt, associated to the same image has been treated as a separated sample."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uK9gp21vQNKx"
      },
      "source": [
        "**The path of the dataset must be adjusted based on the location of the dataset folder!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODIZpuXBQNKx"
      },
      "outputs": [],
      "source": [
        "# adjust based on the location of the dataset folder!\n",
        "refcocog_path =  \"E:/DL_Datasets/refcocog\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O78QC0PAQR9O"
      },
      "source": [
        "If the dataset is on Google Drive, mount the correct point and edit the dataset path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqpapNemQRPI",
        "outputId": "33ebc5e8-2213-410b-b90b-855278b22782"
      },
      "outputs": [],
      "source": [
        "# Google Colab can be imported only if run inside Colab!\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw-JmFgYTyuY"
      },
      "outputs": [],
      "source": [
        "# extract the dataset, ADJUST THE CORRECT LOCATION\n",
        "!tar -xvf ./gdrive/MyDrive/DL/refcocog.tar.gz ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlggCrBCX4Qb"
      },
      "outputs": [],
      "source": [
        "refcocog_path =  \"refcocog\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "snr7hoQUQNKx"
      },
      "source": [
        "Load the pickle file and the instances json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FOSuhz7QNKx",
        "outputId": "9ced374b-d586-4030-8794-7fae5d919224"
      },
      "outputs": [],
      "source": [
        "pick = pickle.load(open(refcocog_path+\"/annotations/refs(umd).p\", \"rb\"))\n",
        "jsn = json.load(open(refcocog_path+\"/annotations/instances.json\", \"rb\"))\n",
        "# set of all images\n",
        "images_set = {}\n",
        "for i in jsn['images']:\n",
        "  image_id = i['id']\n",
        "  images_set[image_id] = i\n",
        "\n",
        "# set of all annotations\n",
        "annotations_set = {}\n",
        "for a in jsn['annotations']:\n",
        "  annotation_id = a['id']\n",
        "  annotations_set[annotation_id] = a\n",
        "\n",
        "# set of all categories\n",
        "categories_set = {}\n",
        "# remap categories ids so that we dont have problems during finetuning of FastRCNN due to missing ids in the range\n",
        "remapper_cat={}\n",
        "max_cat=0\n",
        "for i, c in enumerate(jsn['categories']):\n",
        "  remapper_cat[c[\"id\"]]=i\n",
        "  c[\"id\"] = i\n",
        "  categories_set[c['id']] = c\n",
        "  if c['id'] > max_cat:\n",
        "    max_cat = c['id']\n",
        "print(remapper_cat)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sGp4TjXXQNKy"
      },
      "source": [
        "**Build dataset splits**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBNM2MzEQNKy",
        "outputId": "6007cfb2-8289-49e4-ff76-9867a6e1c839"
      },
      "outputs": [],
      "source": [
        "train_data, train_label       = [], []\n",
        "validate_data, validate_label = [], []\n",
        "test_data, test_label         = [], []\n",
        "for p in pick:\n",
        "    data_image_path = f\"{refcocog_path}/images/{images_set[p['image_id']]['file_name']}\"\n",
        "    data_sentences = p['sentences']\n",
        "    data_bbox = annotations_set[p['ann_id']]['bbox']\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for s in data_sentences:\n",
        "        sentence = s['sent']\n",
        "        data.append([data_image_path, sentence, data_bbox, p[\"category_id\"]])\n",
        "\n",
        "    if p['split'] == 'train':\n",
        "        train_data.extend(data)\n",
        "    elif p['split'] == 'test':\n",
        "        test_data.extend(data)\n",
        "    elif p['split'] == 'val':\n",
        "        validate_data.extend(data)\n",
        "\n",
        "print(f\"train {len(train_data)}, validation {len(validate_data)}, test {len(test_data)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6zjLk_dcQNKy"
      },
      "source": [
        "### Dataset utils methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "455-7Tu5QNKy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def draw_box_on_image(image,size, bbox, color):\n",
        "    w, h = size\n",
        "    p1 = (int(bbox[0]*w), int(bbox[1]*h))\n",
        "    p2 = (int((bbox[0]+bbox[2])*w), int((bbox[1] + bbox[3])*h))\n",
        "    cv2.rectangle(image, p1, p2, color, 3)\n",
        "\n",
        "def compute_target_heatmap(image, box):\n",
        "    img_w, img_h = image.size\n",
        "    x1 = int((box[0]) / img_w * 224)\n",
        "    y1 = int((box[1]) / img_h * 224)\n",
        "    x2 = int((box[0] + box[2]) / img_w * 224)\n",
        "    y2 = int((box[1] + box[3]) / img_h * 224)\n",
        "\n",
        "    target = torch.zeros((224, 224))\n",
        "    target[y1:y2+1, x1:x2+1] = 1\n",
        "    return target\n",
        "\n",
        "def get_batch_data(batch, image_augment=False, augment_p=0.25):\n",
        "    images, target_boxes, prompts, target_heatmaps, categories = [], [], [], [], []\n",
        "    for image_path, prompt, box, cat in batch:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        w, h = image.size\n",
        "        if image_augment and random.random()<augment_p:\n",
        "            augment_transform = transforms.Compose([\n",
        "                transforms.ColorJitter(brightness=(0.1,0.6), contrast=(0.4, 1),saturation=(0, 0.4), hue=(-0.5, 0.5))\n",
        "            ])\n",
        "            image = augment_transform(image)\n",
        "\n",
        "        correct_box = [box[0] / w, box[1] / h, box[2] / w, box[3] / h]\n",
        "        target_boxes.append(correct_box)\n",
        "        categories.append(cat)\n",
        "        images.append(image)\n",
        "        prompts.append(prompt)\n",
        "        target_heatmaps.append(compute_target_heatmap(image, box))\n",
        "    target_boxes = torch.tensor(target_boxes).to(device)\n",
        "    target_boxes.requires_grad=False\n",
        "    target_heatmaps = torch.stack(target_heatmaps).to(device)\n",
        "    target_heatmaps.requires_grad=False\n",
        "    return images, prompts, target_boxes, target_heatmaps, categories\n",
        "\n",
        "def view_image_with_bbox(image_path, prompt, bbox):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    image = np.asarray(image)\n",
        "\n",
        "    p1 = (int(bbox[0]), int(bbox[1]))\n",
        "    p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
        "    print(bbox)\n",
        "    print(f\"normalized: {[bbox[0]/w, bbox[1]/h, bbox[2]/w, bbox[3]/h]}\")\n",
        "    print(f\"p1 {p1}, p2 {p2}\")\n",
        "    cv2.rectangle(image, p1, p2, (0,255,255), 3)\n",
        "\n",
        "    plt.imshow(image)\n",
        "    plt.title(prompt)\n",
        "    plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rYUe8K7AQNKy"
      },
      "source": [
        "# 1. Using transfer learning: FastRCNN\n",
        "\n",
        "The pipeline is to feed the image to Fast-RCNN that returns a set of bounding boxes.\n",
        "\n",
        "Each subimage corresponding to each bounding box is extracted and then the similarity between the CLIP's encoding of the subimage and the prompt is computed. When Fast-RCNN does not return a bounding box, we use the entire image as the bounding box.\n",
        "\n",
        "The bounding box with the highest similarity is returned as output.\n",
        "\n",
        "![arch](https://drive.google.com/uc?id=1nIj5e2Bo2wbgnndUJrZCRIEmdOTaBGLi)\n",
        "\n",
        "\n",
        "Various runs of the model have achieved a general GIoU of around `0.7`\n",
        "\n",
        "- **GIoU**: ~ `0.7`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TXY2Cw7lQNKz"
      },
      "source": [
        "### Dataset class\n",
        "\n",
        "This dataset class is created to adapt the RefCOCOg data to Fast-RCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-8mVzh2QNKz"
      },
      "outputs": [],
      "source": [
        "class RefCOCODataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir,pick, jsn, transforms=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "        # load the annotations file, it also contain information of image names\n",
        "        # load annotations\n",
        "        self.pick = pick\n",
        "        self.jsn = jsn\n",
        "\n",
        "        set = {}\n",
        "        for a in jsn['annotations']:\n",
        "            if set.get(a[\"image_id\"]) is None:\n",
        "                set[a[\"image_id\"]] = {}\n",
        "            bbox = a[\"bbox\"]\n",
        "            label = categories_set[remapper_cat[a[\"category_id\"]]][\"id\"]\n",
        "\n",
        "            if set.get(a[\"image_id\"]).get(\"bboxes\") is None:\n",
        "                set[a[\"image_id\"]][\"bboxes\"] = []\n",
        "            if set.get(a[\"image_id\"]).get(\"labels\") is None:\n",
        "                set[a[\"image_id\"]][\"labels\"] = []\n",
        "            b = box_convert(torch.tensor(bbox), in_fmt=\"xywh\",out_fmt=\"xyxy\")\n",
        "            set[a[\"image_id\"]][\"bboxes\"].append(b.tolist())\n",
        "            set[a[\"image_id\"]][\"labels\"].append(label)\n",
        "\n",
        "        for i in jsn['images']:\n",
        "            image_id = i['id']\n",
        "            set[image_id][\"file_name\"] = i[\"file_name\"]\n",
        "            set[image_id][\"image_id\"] = image_id\n",
        "        self.elems = list(set.values())\n",
        "        self.tensor = torchvision.transforms.ToTensor()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        cur = self.elems[idx]\n",
        "        # get the image path from the annoations data\n",
        "        img = Image.open(self.data_dir +\"/images/\"+cur[\"file_name\"]).convert(\"RGB\")\n",
        "        img_res = self.tensor(img).to(device)\n",
        "\n",
        "        boxes = cur[\"bboxes\"]\n",
        "        num_objs = len(boxes)\n",
        "        labels = cur[\"labels\"]\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).to(device)\n",
        "        labels = torch.as_tensor(labels).to(device)\n",
        "\n",
        "        image_id = torch.tensor([cur[\"image_id\"]]).to(device)\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]).to(device)\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs), dtype=torch.int64).to(device)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img_res, target = self.transforms(img_res, target)\n",
        "\n",
        "        return  img_res, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.elems)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teoXXci8QNKz"
      },
      "outputs": [],
      "source": [
        "def get_batch(dataset, start, size):\n",
        "    images = []\n",
        "    targets = []\n",
        "    for i in range(start, start+size):\n",
        "        images.append(dataset[i][0])\n",
        "        targets.append(dataset[i][1])\n",
        "    return images, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwFiHFUAQNKz"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "dataset = RefCOCODataset(refcocog_path, pick, jsn)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z4ZL18bFQNKz"
      },
      "source": [
        "### Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie3UAlmyQNKz",
        "outputId": "7dac0d78-84e1-4091-9d05-371c4c256fac"
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# change head to finetune\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 80)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWI-W5CjQNKz"
      },
      "outputs": [],
      "source": [
        "new_params = []\n",
        "pre_params = []\n",
        "for p in model.named_parameters():\n",
        "    if not p[1].requires_grad: continue\n",
        "    if \"roi_heads.box_predictor\" in p[0]:\n",
        "        new_params.append(p[1])\n",
        "    else:\n",
        "        pre_params.append(p[1])\n",
        "\n",
        "optimizer = torch.optim.SGD([\n",
        "    {\"params\":pre_params, \"lr\":5E-3},\n",
        "    {\"params\":new_params, \"lr\":1E-2}\n",
        "        ], momentum=0.9, weight_decay=0.0002)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=4, gamma=0.25)\n",
        "\n",
        "epochs = 16\n",
        "batch_size = 8\n",
        "# train for 50% of the dataset elements\n",
        "size = int(len(dataset)*0.5)\n",
        "\n",
        "model.train()\n",
        "with open(\"finetuning.csv\", \"w\", newline=\"\") as file:\n",
        "    csvwriter = csv.writer(file)\n",
        "    csvwriter.writerow([\"epoch\", \"loss\"])\n",
        "for e in range(epochs):\n",
        "    losses = []\n",
        "    for i in tqdm(range(0, size, batch_size)):\n",
        "        optimizer.zero_grad()\n",
        "        images, targets = get_batch(dataset, i, batch_size)\n",
        "        pred = model(images, targets)\n",
        "        l = sum(loss for loss in pred.values())\n",
        "        losses.append(l.item())\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "    with open(\"finetuning.csv\", \"a\", newline=\"\") as file:\n",
        "        csvwriter = csv.writer(file)\n",
        "        csvwriter.writerow([e, sum(losses)/len(losses)])\n",
        "    print(f\"epoch {e} | loss {sum(losses)/len(losses)}\")\n",
        "\n",
        "torch.save(model, \"fastrcnn.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPsEPsScQNK0"
      },
      "outputs": [],
      "source": [
        "def predict(img_path, model, threshold=0.1):\n",
        "  img = Image.open(img_path)\n",
        "  w,h = img.size\n",
        "  transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "  img = transform(img).to(device)\n",
        "  pred = model([img])\n",
        "\n",
        "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().cpu().numpy())]\n",
        "  pred_score = list(pred[0]['scores'].detach().cpu().numpy())\n",
        "  pred_t = [pred_score.index(x) for x in pred_score if x>threshold]\n",
        "  if len(pred_t) <=0:\n",
        "    return [[(0,0), (w,h)]]\n",
        "  pred_t = pred_t[-1]\n",
        "  pred_boxes = pred_boxes[:pred_t+1]\n",
        "  return pred_boxes\n",
        "\n",
        "def predict_and_show(img_path, threshold=0.5, rect_th=3, text_size=1, text_th=3):\n",
        "  boxes, pred_cls = predict(img_path, threshold)\n",
        "  img = cv2.imread(img_path)\n",
        "  img = np.array(img)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  for i in range(len(boxes)):\n",
        "    cv2.rectangle(img, (int(boxes[i][0][0]),int(boxes[i][0][1])), (int(boxes[i][1][0]),int(boxes[i][1][1])), (0, 255, 0), rect_th)\n",
        "    cv2.putText(img,pred_cls[i], (int(boxes[i][0][0]),int(boxes[i][0][1])), cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
        "    plt.figure(figsize=(10,8))\n",
        "  plt.imshow(img)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qWr70Rz_eBs8"
      },
      "source": [
        "Evaluate the model on the whole dataset.\n",
        "This requires huge amount of time as for each image, the similarity with each bounding box must be calculated with CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_lq7uSJQNK0"
      },
      "outputs": [],
      "source": [
        "execute = True\n",
        "#model = torch.load(\"fastrcnn.pt\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "data = train_data + validate_data+ test_data\n",
        "if execute:\n",
        "    base_clip, base_clip_preprocess = clip.load(\"RN50\")\n",
        "    base_clip = base_clip.to(device)\n",
        "    losses = []\n",
        "\n",
        "    for idx, data in enumerate(tqdm(data)):\n",
        "        image_path, prompt, target_box, labels = data\n",
        "        image =  Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        image_copy = np.asarray(image)\n",
        "        cropped_images = []\n",
        "\n",
        "        boxes = predict(image_path, model)\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            target_tensor = torch.tensor(target_box).to(device)\n",
        "            h, w = image.size\n",
        "            ans = torch.tensor([0, 0, h, w]).to(device)\n",
        "            loss = generalized_box_iou_loss(ans, target_tensor)\n",
        "            losses.append(loss.item())\n",
        "            continue\n",
        "\n",
        "        for ans_box in boxes:\n",
        "            (x1, y1), (x2, y2) = ans_box[:4]\n",
        "            x1 = int(x1); y1 = int(y1); x2 = int(x2); y2 = int(y2)\n",
        "            cropped_image = image_copy[y1:y2, x1:x2]\n",
        "            cropped_images.append(cropped_image)\n",
        "\n",
        "        preprocessed_images = []\n",
        "        for img_np in cropped_images:\n",
        "            img = Image.fromarray(img_np)\n",
        "            preprocessed_image = base_clip_preprocess(img)\n",
        "            preprocessed_images.append(preprocessed_image)\n",
        "\n",
        "        cropped_image_tensors = torch.stack(preprocessed_images).to(device)\n",
        "\n",
        "        text_tokens = clip.tokenize(prompt).to(device)\n",
        "        text_tokens.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = base_clip.encode_image(cropped_image_tensors).float()\n",
        "            text_features = base_clip.encode_text(text_tokens).float()\n",
        "\n",
        "        # divide by norm\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        similarity = torch.matmul(image_features, text_features.T)\n",
        "        #similarity\n",
        "\n",
        "        ans = similarity.argmax()\n",
        "        (x1, y1), (x2, y2) = boxes[ans][:4]\n",
        "\n",
        "        target_tensor = torch.tensor(target_box).to(device)\n",
        "        target_tensor = box_convert(target_tensor, in_fmt=\"xywh\", out_fmt=\"xyxy\")\n",
        "\n",
        "        loss = generalized_box_iou_loss(torch.tensor([x1,y1,x2,y2]).to(device), target_tensor)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    print(f\"GIoU: {sum(losses) / len(losses)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jc616MIhQNK0"
      },
      "source": [
        "# 2. A novel single stage proposal: adapting CLIP itself\n",
        "\n",
        "![arch](https://drive.google.com/uc?id=1Qt6PiPXNQmalGTVVkCfA3_VPMAvcwNL8)\n",
        "\n",
        "The architecture proposed, is inspired by the recent work: [Adapting CLIP For Phrase Localization Without Further Training](https://arxiv.org/abs/2204.03647) by _Jiahao Li, Greg Shakhnarovich, Raymond A. Yeh_.\n",
        "\n",
        "In their paper, the goal was to adapt the **CLIP** model to phrase localization without the need of any further training. This goal is a perfect starting point in order to adapt their solution to out phrase grounding task for the project.\n",
        "\n",
        "To begin with, we have taken the \"backbone\" model from their [repository](https://github.com/pals-ttic/adapting-CLIP).\n",
        "The code borrowed includes the modification of the ResNet of CLIP in order to introduce, in the last layer, a spatial attention layer.\n",
        "The goal is to adapt the CLIP model for phrase localization and, since CLIP effectively acts as an image and text embedder, there is the need to adapt the model to introduce spatial reasoning.\n",
        "The steps are:\n",
        "- Extract spatial features **mantaining their semantic meaning** (alignment with text)\n",
        "- Compute the inner product with the text embedding, effectively generating a score map (heatmap)\n",
        "\n",
        "-----\n",
        "\n",
        "**Single stage model**\n",
        "\n",
        "Many proposed methods (such as FastRCNN proposed above) that attempts to perform transfer learning to phrase localization and grounding starting from famous conv networks such as Fast-RCNN or Mask-RCNN, implements a what is called **two stage models** [2].\n",
        "\n",
        "Two stage models entails the use of an external feature extractor such as a Convolutional neural network that usually performs object detection and is able to extract bounding boxes.\n",
        "These candidates boxes are then fed to the CLIP image encoder and compared with the encoding of the prompt, outputting the box with the highest score.\n",
        "These solutions have an important caveat: since the CLIP encoding is compared with the subimage composed of the bounding box only, spatial reasoning is not included into the model.\n",
        "\n",
        "The proposed method can be instead considered a **one stage model**.\n",
        "Apart from the performance that we were able to achieve, the model should in theory be able to convey the spatial reasoning into the box regressor.\n",
        "As shown below, the CLIP model is slightly modified in order to include this feature. This is the reason why the model is able to infer the heatmaps with quite remarkable accuracy.\n",
        "\n",
        "\n",
        "It is worth noticing that, although the heatmaps are extracted with the `ResNetHighResV2` model and fed into the `HeatmapToBox` regressor, the framework can be considered as a single stage.\n",
        "Both the models could be infact unified in order to provide a single interface. This has not been done in order to better delineate the original contribution of the paper and our contribution.\n",
        "\n",
        "##### **This proposed methods was developed as another pipeline. We were not able to train the model extensively on the dataset as we finished the Azure server resources quite fast.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Sue0-ww4QNK0"
      },
      "source": [
        "#### Custom Spatial CLIP\n",
        "\n",
        "In this cells, the code inspired from **Adapting CLIP For Phrase Localization Without Further Training** is introduced.\n",
        "It includes the customized `AttentionSpatial2d` which basically introduces spatial attention.\n",
        "\n",
        "The `ModifiedSpatialResNet` is a simple ResNet which includes the previously introduced `AttentionSpatial2d`.\n",
        "\n",
        "Finally, the custom CLIP with the `ModifiedSpatialResNet` is defined.\n",
        "\n",
        "The final method `build_feature_extractor_model` has the job of creating this customized CLIP and to enable **transfer learning** from CLIP by copying the relative weights into the custom model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VSLnz2GQNK0"
      },
      "outputs": [],
      "source": [
        "clip_model, clip_preprocess = clip.load(\"RN50\",jit=False,device=device)\n",
        "clip_model = clip_model.to(device)\n",
        "\n",
        "def linear(x, weight, bias):\n",
        "    x = x.matmul(weight.t())\n",
        "    x += bias\n",
        "    return x\n",
        "\n",
        "class AttentionSpatial2d(AttentionPool2d):\n",
        "    \"\"\"Edited attention pool layer to introduce spatial attention\"\"\"\n",
        "    def __init__(self,\n",
        "                 spacial_dim: int,\n",
        "                 embed_dim: int,\n",
        "                 num_heads: int,\n",
        "                 output_dim: int = None):\n",
        "        super().__init__(spacial_dim, embed_dim, num_heads, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n, c, h, w = x.shape\n",
        "        x = x.reshape(n, c, h*w).permute(2, 0, 1) # NCHW -> (H*W)NC\n",
        "        x = linear(x, self.v_proj.weight, self.v_proj.bias)\n",
        "        x = linear(x, self.c_proj.weight, self.c_proj.bias)\n",
        "        x = x.permute(1, 2, 0).reshape(n, -1, h, w) # (H*W)NC -> C(H*W)N -> (N, -1, H, W)\n",
        "        return x\n",
        "\n",
        "class ModifiedSpatialResNet(ModifiedResNet):\n",
        "    \"\"\"Modified resnet to include the edited attention pool layer\"\"\"\n",
        "    def __init__(self,\n",
        "                 layers,\n",
        "                 output_dim,\n",
        "                 heads,\n",
        "                 input_resolution=224,\n",
        "                 width=64):\n",
        "        super().__init__(layers, output_dim, heads, input_resolution, width)\n",
        "\n",
        "        self.attnpool = AttentionSpatial2d(\n",
        "            input_resolution // 32, width * 32, heads, output_dim)\n",
        "\n",
        "class CLIPSpatialResNet(CLIP):\n",
        "    \"\"\"Modified spatial CLIP including the spatial attention\"\"\"\n",
        "    def __init__(self,\n",
        "                 embed_dim: int,\n",
        "                 # vision\n",
        "                 image_resolution: int,\n",
        "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
        "                 vision_width: int,\n",
        "                 vision_patch_size: int,\n",
        "                 # text\n",
        "                 context_length: int,\n",
        "                 vocab_size: int,\n",
        "                 transformer_width: int,\n",
        "                 transformer_heads: int,\n",
        "                 transformer_layers: int):\n",
        "\n",
        "        super().__init__(embed_dim, image_resolution, vision_layers, vision_width,\n",
        "                         vision_patch_size, context_length, vocab_size,\n",
        "                         transformer_width, transformer_heads, transformer_layers)\n",
        "\n",
        "        # Override the visual model\n",
        "        vision_heads = vision_width * 32 // 64\n",
        "        self.visual = ModifiedSpatialResNet(layers=vision_layers,\n",
        "                                            output_dim=embed_dim,\n",
        "                                            heads=vision_heads,\n",
        "                                            input_resolution=image_resolution,\n",
        "                                            width=vision_width)\n",
        "\n",
        "\n",
        "    def forward(self, image):\n",
        "        image = image.type(self.dtype)\n",
        "\n",
        "        # pad image\n",
        "        pad = 64\n",
        "        pad = (pad, pad, pad, pad)\n",
        "        padded_image = F.pad(image, pad, \"constant\", 0)\n",
        "\n",
        "        # get features\n",
        "        features = self.encode_image(padded_image)\n",
        "        target_size_h, target_size_w = image.size(-2) // 32, image.size(-1) // 32\n",
        "\n",
        "        # compute new pad size\n",
        "        pad_h = (features.size(-2) - target_size_w) // 2\n",
        "        pad_w = (features.size(-1) - target_size_w) // 2\n",
        "        features = features[:, :, pad_h:pad_h+target_size_h, pad_w:pad_w+target_size_w]\n",
        "\n",
        "        # interpolate back to 224*224\n",
        "        features = F.upsample(features, size=(image.size(-2), image.size(-1)),\n",
        "            mode=\"bilinear\", align_corners=None) # 1*C*H*W\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "def build_feature_extractor_model(clip_model):\n",
        "    \"\"\"\"Instantiate the modified CLIP model and adapt weights\"\"\"\n",
        "    # transfer learning: extract weights from CLIP\n",
        "    clip_state_dict = clip_model.state_dict()\n",
        "    # run [k for k in clip_state_dict if k.startswith(\"visual.layer2\")] to see what's up\n",
        "    counts: list = [len(set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n",
        "    vision_layers = tuple(counts)\n",
        "    vision_width = clip_state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
        "    output_width = round(\n",
        "        (clip_state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
        "\n",
        "    vision_patch_size = None\n",
        "    image_resolution = output_width * 32\n",
        "\n",
        "    embed_dim = clip_state_dict[\"text_projection\"].shape[1]\n",
        "    context_length = clip_state_dict[\"positional_embedding\"].shape[0]\n",
        "    vocab_size = clip_state_dict[\"token_embedding.weight\"].shape[0]\n",
        "    transformer_width = clip_state_dict[\"ln_final.weight\"].shape[0]\n",
        "    transformer_heads = transformer_width // 64\n",
        "    transformer_layers = len(set(\n",
        "        k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"transformer.resblocks\")))\n",
        "\n",
        "    model = CLIPSpatialResNet(\n",
        "        embed_dim,\n",
        "        image_resolution, vision_layers, vision_width, vision_patch_size,\n",
        "        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers)#.to(device)\n",
        "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
        "        if key in clip_state_dict:\n",
        "            del clip_state_dict[key]\n",
        "\n",
        "\n",
        "    # False for the average filter layer.\n",
        "    model.load_state_dict(clip_state_dict, strict=False)\n",
        "    model.eval()\n",
        "    if device == 'cpu':\n",
        "        model.float()\n",
        "    for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "    # for param in model.visual.attnpool.parameters():\n",
        "    #     param.requires_grad = True\n",
        "\n",
        "    #convert_weights(model)\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LwRNQGGPQNK1"
      },
      "source": [
        "#### The Feature extractor: Heatmaps generator\n",
        "\n",
        "Here, the actual feature extractor model is created.\n",
        "\n",
        "The `ResNetHighResV2` model is responsible for taking as input images and prompts, generating the scoremaps (heatmaps).\n",
        "\n",
        "> The model has been adapted and customized (V2) in order to enable batch computation of heatmaps, since in the original implementation of the paper, the model was able to accept single image and prompt.\n",
        "\n",
        "-----\n",
        "Example of an image and its corresponding heatmap. Notice that the heatmaps is _shrinked_ and normalized to a fixed width and height, nonetheless spatial reasoning is mantained.\n",
        "\n",
        "`the white imac computer that is also turned on`\n",
        "\n",
        "![1.png](https://drive.google.com/uc?id=1FoXl6V10CbmBfBgsrZeadRLySzBmwsnd)![h_1.png](https://drive.google.com/uc?id=1McXB533mIJt6_w3ylJ4ljisuZiVhIwXh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGcaemYZQNK1"
      },
      "outputs": [],
      "source": [
        "class ResNetHighResV2(nn.Module):\n",
        "    \"\"\"Feature extractor that includes CLIP as its fundation model\"\"\"\n",
        "    def __init__(self, clip_preprocess, clip_model, tokenize, temperature=0.1, remap_heatmaps=True):\n",
        "        super().__init__()\n",
        "        self.spatial_model = build_feature_extractor_model(clip_model)\n",
        "        self.clip_preprocess = clip_preprocess\n",
        "        self.tokenize = tokenize\n",
        "        self.temperature = temperature\n",
        "        self.remap_heatmaps=remap_heatmaps\n",
        "\n",
        "    def get_image_features(self, images):\n",
        "        images = [clip_preprocess(image) for image in images]\n",
        "        images = torch.stack(images).to(device)\n",
        "        image_features = self.spatial_model(images)\n",
        "        return image_features\n",
        "\n",
        "    def get_text_features(self, texts):\n",
        "        tokenized_texts = self.tokenize(texts).to(device)\n",
        "        text_features = self.spatial_model.encode_text(tokenized_texts)\n",
        "        return text_features\n",
        "\n",
        "    def get_heatmaps(self, image_features, text_features):\n",
        "        heatmaps = ((image_features / image_features.norm(dim=1, keepdim=True)) * (text_features / text_features.norm(dim=1, keepdim=True))[:, :, None, None]).sum(1)\n",
        "        heatmaps = torch.exp(heatmaps/self.temperature)\n",
        "        if self.remap_heatmaps:\n",
        "            norm_heatmaps = torch.tensor(heatmaps)\n",
        "            for i in range(len(heatmaps)):\n",
        "                min = torch.min(heatmaps[i])\n",
        "                norm_heatmaps[i] = (heatmaps[i] - min) / (torch.max(heatmaps[i])-min) + 1E-3\n",
        "            heatmaps = norm_heatmaps\n",
        "        heatmaps = heatmaps.pow(5)\n",
        "        return heatmaps\n",
        "\n",
        "    def forward(self, images, texts):\n",
        "        image_features = self.get_image_features(images)\n",
        "        text_features = self.get_text_features(texts)\n",
        "        heatmaps = self.get_heatmaps(image_features, text_features)\n",
        "        return heatmaps.squeeze(dim=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QkK-2LDhQNK1"
      },
      "source": [
        "### Bounding Box regressor\n",
        "\n",
        "The following model is the **head** of all of our framework.\n",
        "\n",
        "It takes as input the heatmaps generated by `ResNetHighResV2` and regresses the four points of the bounding box in the form `(x, y, w, h)`.\n",
        "\n",
        "It is worth noticing that this choice of a single bounding box regression without label is motivated by the fact that in the **RefCOCOg** dataset, each (image, prompt) is associated to a single bounding box.\n",
        "Hence, since the job was to generate a phrase localization framework on the **RefCOCOg** dataset, the choice was to have a regressor that simply outputs a single box.\n",
        "\n",
        "-----\n",
        "\n",
        "The model effectively treats the heatmap as an image.\n",
        "\n",
        "The first sequential layer is a series of `Conv2d` layers, that have the job to comprehend the spatial structure of the heatmap.\n",
        "\n",
        "The second sequential layer is composed of an `AvgPool1d` that has the job to smooth the heatmap, followed by a **FFNN** that effectively regresses the four points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6tuujOlQNK1"
      },
      "outputs": [],
      "source": [
        "class HeatmapToBox(nn.Module):\n",
        "    \"\"\"Custom model to regress a bounding box from an heatmap\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Conv2d(1,1,9,stride=1),\n",
        "            nn.Conv2d(1,1,7,stride=2),\n",
        "            nn.Conv2d(1,1,3,stride=2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.AvgPool1d(4),\n",
        "\n",
        "            nn.Linear(676, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Sigmoid(),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Sigmoid(),\n",
        "\n",
        "            nn.Linear(128, 4),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        print(f\"bboxer parameters: {self.params_count()}\")\n",
        "\n",
        "    def params_count(self):\n",
        "        c = 0\n",
        "        for p in self.parameters():\n",
        "            c += p.numel()\n",
        "        return c\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.seq(x.unsqueeze(dim=1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s_PHZRquQNK1"
      },
      "source": [
        "## Training\n",
        "\n",
        "The training process is divided into two:\n",
        "- Finetuning the custom **Spatial CLIP**     \n",
        "    This step aims in enforcing the spatial CLIP to generate good heatmaps. The loss is a `MSELoss` with respect to binary target heatmaps. These targets are basically heatmaps with the value 1 only on pixels inside the bounding box and 0 otherwise.\n",
        "    \n",
        "    **The pipeline code is present below, however we noticed that finetuning even only the last layer of the `ModifiedResNet` model requires huge amount of computational time, even when other layers are frozen. Therefore extensive training is not been carried out.**\n",
        "    \n",
        "- Training the **Box Regressor**     \n",
        "    In this step we train our custom model to regress the actual bounding box\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dr1HsM-qQNK1"
      },
      "source": [
        "**Intersection Over Union**\n",
        "\n",
        "The following function from `torchvision` calculates a gradient friendly version of the [Generalized Intersection Over Union](https://giou.stanford.edu/) and treats it as a loss, meaning that the `torch.Tensor` result is reduced to a single float that represents the average value for all the bounding boxes.\n",
        "\n",
        "Boxes are converted from the **RefCOCOg** format to the format required by `torchvision`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHV5wCzmQNK1"
      },
      "outputs": [],
      "source": [
        "def iou(boxes1, boxes2) -> torch.Tensor:\n",
        "    return generalized_box_iou_loss(box_convert(boxes1,in_fmt=\"xywh\",out_fmt=\"xyxy\"),box_convert(boxes2,in_fmt=\"xywh\",out_fmt=\"xyxy\"),reduction=\"mean\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cEPB0bTwQNK2"
      },
      "source": [
        "**Training parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqZ1sFgLQNK2"
      },
      "outputs": [],
      "source": [
        "train_size = 128\n",
        "train_batch_size = 8\n",
        "epochs = 64\n",
        "mini_train_data = train_data[:train_size]\n",
        "\n",
        "validation_size = 512\n",
        "validation_batch_size = 16\n",
        "mini_val_data = validate_data[:validation_size]\n",
        "\n",
        "test_size = 256\n",
        "test_batch_size = 16\n",
        "mini_test_data = test_data[:test_size]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8VuD1i8EQNK2"
      },
      "source": [
        "#### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ClkYoriQNK2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def evaluate_batch_routine(model, loss_fn, feature_extractor,data_batch, graphical=False, save=False):\n",
        "    %matplotlib inline\n",
        "    if save:\n",
        "        try:\n",
        "            os.mkdir(\"imgs\")\n",
        "        except OSError as error:\n",
        "            pass\n",
        "    model.eval()\n",
        "    images, prompts, target_boxes, target_heatmaps = get_batch_data(data_batch)\n",
        "    with torch.no_grad():\n",
        "        heatmaps = feature_extractor(images, prompts)\n",
        "        prediction_boxes = model(heatmaps.to(device))\n",
        "\n",
        "    c=0\n",
        "    if graphical:\n",
        "        for img, p, heatmap, correct, predicted in zip(images, prompts, heatmaps, target_boxes, prediction_boxes):\n",
        "            size = img.size\n",
        "            img_arr = np.asarray(img)\n",
        "            draw_box_on_image(img_arr, size, predicted, (255,0,0))\n",
        "            draw_box_on_image(img_arr, size, correct, (0,255,0))\n",
        "\n",
        "            f = plt.figure(figsize=(12,6))\n",
        "            plt.title(p)\n",
        "            plt.axis(\"off\")\n",
        "            ax=f.add_subplot(1, 2, 1)\n",
        "            ax.imshow(img_arr)\n",
        "            ax.axis(\"off\")\n",
        "            ax=f.add_subplot(1, 2, 2)\n",
        "            ax.imshow(heatmap.cpu())\n",
        "            ax.axis(\"off\")\n",
        "            if save:\n",
        "                plt.savefig(f\"imgs/{c}.png\")\n",
        "            plt.show()\n",
        "            c += 1\n",
        "\n",
        "        print(f\"correct {correct}, predict: {predicted}\")\n",
        "\n",
        "    loss = loss_fn(prediction_boxes, target_boxes)\n",
        "    return loss.item(), iou(prediction_boxes, target_boxes).item()\n",
        "\n",
        "def training_routine(model, loss_fn, feature_extractor, optimizer):\n",
        "    model.train()\n",
        "    epoch_loss =[]\n",
        "    giou = []\n",
        "    for i in tqdm(range(0, train_size, train_batch_size)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_data = mini_train_data[i:i+train_batch_size]\n",
        "        images, prompts, target_boxes, target_heatmaps = get_batch_data(batch_data)\n",
        "        with torch.no_grad():\n",
        "            pred_heatmaps = feature_extractor(images, prompts)\n",
        "        prediction_boxes = model(pred_heatmaps.to(device))\n",
        "\n",
        "        loss = loss_fn(prediction_boxes, target_boxes)\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "        giou.append(iou(prediction_boxes, target_boxes).item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return sum(epoch_loss) / len(epoch_loss), sum(giou) / len(giou)\n",
        "\n",
        "def validation_routine(model, loss_fn, feature_extractor):\n",
        "    model.eval()\n",
        "    epoch_loss =[]\n",
        "    giou = []\n",
        "    for i in tqdm(range(0, validation_size, validation_batch_size)):\n",
        "        batch_data = mini_val_data[i:i+validation_batch_size]\n",
        "        images, prompts, target_boxes, target_heatmaps = get_batch_data(batch_data)\n",
        "        with torch.no_grad():\n",
        "            pred_heatmaps = feature_extractor(images, prompts)\n",
        "            prediction_boxes = model(pred_heatmaps.to(device))\n",
        "            loss = loss_fn(prediction_boxes, target_boxes)\n",
        "            epoch_loss.append(loss.item())\n",
        "            giou.append(iou(prediction_boxes, target_boxes).item())\n",
        "\n",
        "    return sum(epoch_loss) / len(epoch_loss), sum(giou) / len(giou)\n",
        "\n",
        "def finetune_clip_routine(loss_fn, feature_extractor, optimizer):\n",
        "    epoch_loss =[]\n",
        "    for i in range(0, train_size, train_batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        batch_data = mini_train_data[i:i+train_batch_size]\n",
        "        images, prompts, target_boxes, target_heatmaps = get_batch_data(batch_data)\n",
        "        heatmaps = feature_extractor(images, prompts)\n",
        "        rem_loss = loss_fn(heatmaps, target_heatmaps)\n",
        "        epoch_loss.append(rem_loss.item())\n",
        "\n",
        "        rem_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return sum(epoch_loss) / len(epoch_loss)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_gvqWBX7QNK2"
      },
      "source": [
        "### Training Cycle"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Qvdz06QNK2"
      },
      "source": [
        "#### Finetuning the Spatial CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7PH477fQNK2"
      },
      "outputs": [],
      "source": [
        "feature_extractor = ResNetHighResV2(clip_preprocess, clip_model, clip.tokenize, remap_heatmaps=True, temperature=0.35).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwuOyccAQNK2"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "finetune = False\n",
        "if finetune:\n",
        "    optimizer = torch.optim.Adam(params=feature_extractor.spatial_model.visual.attnpool.parameters(), lr=4E-6)\n",
        "    feature_extractor.eval()\n",
        "    feature_extractor.spatial_model.visual.attnpool.train()\n",
        "    for epoch in range(epochs):\n",
        "        loss = finetune_clip_routine(loss_fn ,feature_extractor, optimizer)\n",
        "        print(f\"epoch {epoch}\")\n",
        "        print(f\"loss: {loss}\")\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    feature_extractor.eval()\n",
        "    torch.save(feature_extractor, \"extractor.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCNJhtIjQNK3"
      },
      "outputs": [],
      "source": [
        "feature_extractor = torch.load(\"extractor.pt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0IwGCdp9QNK3"
      },
      "source": [
        "#### Training the box regressor\n",
        "\n",
        "> We finished available hours of the Azure server quite fast, due to testing and training of old pipelines, the current training process has been carried out on around **1000** elements on our personal PC.\n",
        "\n",
        "As can be seen the model overfits the data very fast, due to the fact that the training data is very small, around 1000 elements.\n",
        "\n",
        "The overfit point is dependent of mainly the dimension of the dataset.\n",
        "We believe that in situations in which the dataset is successfully large, the model could be able to effectively represent data, as intentionally overfitting small dataset allowed us to achieve under `0.5` GIoU.\n",
        "\n",
        "In the testing environment, the model reached around `0.8` GIoU by only training on a dataset of 1000 elements.\n",
        "\n",
        "In order to improve the current pipeline, suggestions are presented at the end of the notebook, in the Future Work section.\n",
        "\n",
        "![image-2.png](https://drive.google.com/uc?id=1gdd3VADEz4EI84hw8L4oPFLNLDFg9hV2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bXDRifCQNK3"
      },
      "outputs": [],
      "source": [
        "bboxer = HeatmapToBox().to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(params=bboxer.parameters(), lr=4E-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXj-J-K4QNK3"
      },
      "outputs": [],
      "source": [
        "best = 1E3\n",
        "best_giou = 1E4\n",
        "val_loss = val_giou = 1E3\n",
        "with open(\"training.csv\", \"w\", newline=\"\") as file:\n",
        "    csvwriter = csv.writer(file)\n",
        "    csvwriter.writerow([\"epoch\", \"loss\", \"giou\",\"val_loss\",\"val_giou\"])\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # if epoch % 2 == 0:\n",
        "    #     val_loss, val_giou = validation_routine(bboxer, loss_fn, feature_extractor)\n",
        "    loss, giou = training_routine(bboxer, loss_fn, feature_extractor, optimizer)\n",
        "    with open(\"training.csv\", \"a\", newline=\"\") as file:\n",
        "        csvwriter = csv.writer(file)\n",
        "        csvwriter.writerow([epoch, loss, giou, val_loss, val_giou])\n",
        "\n",
        "    print(f\"epoch {epoch} | loss {loss} | giou {giou} | val_loss {val_loss} | val_giou {val_giou}\")\n",
        "\n",
        "    if best_giou > val_giou:\n",
        "        torch.save(bboxer, \"checkpoint.pt\")\n",
        "        best_giou=val_giou\n",
        "torch.save(bboxer, \"regressor.pt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uiE4siSkQNK4"
      },
      "source": [
        "Plot and save images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkc2AzPGQNK4"
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline\n",
        "# df = pd.read_csv(\"training.csv\")\n",
        "\n",
        "# f = plt.figure(figsize=(12,6))\n",
        "\n",
        "# ax=f.add_subplot(1, 2, 1)\n",
        "# ax.set_title(\"MSELoss\")\n",
        "# ax.plot(df.epoch, df.loss)\n",
        "# ax.plot(df.epoch, df.val_loss.rolling(10).mean())\n",
        "# ax.legend([\"training\",\"validation\"], loc=3)\n",
        "\n",
        "# ax=f.add_subplot(1, 2, 2)\n",
        "# ax.set_title(\"GIoU\")\n",
        "# ax.plot(df.epoch, df.giou)\n",
        "# ax.plot(df.epoch, df.val_giou.rolling(10).mean())\n",
        "# ax.legend([\"training\",\"validation\"], loc=3)\n",
        "\n",
        "# plt.savefig(\"losses.png\")\n",
        "# plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PsQfjRsUQNK4"
      },
      "source": [
        "Save the model manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYeIFjt3QNK4"
      },
      "outputs": [],
      "source": [
        "torch.save(bboxer, \"regressor.pt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wNzMQjNCQNK4"
      },
      "source": [
        "Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy7NIsbhQNK4"
      },
      "outputs": [],
      "source": [
        "bboxer = torch.load(\"regressor.pt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vBx5C6IoQNK4"
      },
      "source": [
        "## Testing and evaluating\n",
        "\n",
        "> The presented results have been achieved on the training set\n",
        "\n",
        "As the results show, the model is able to represent the data, we tested by intetionally overfitting small datasets.\n",
        "\n",
        "- MSELoss: `5.7E-3`\n",
        "- GIoU : `0.51`\n",
        "\n",
        "\n",
        "-----\n",
        "\n",
        "\n",
        "![14.png](https://drive.google.com/uc?id=1-jgR4rHJtjgy70MGwadBRGzbqunDnh-c)\n",
        "![12.png](https://drive.google.com/uc?id=1LY99Y-dcGFvWgeWAwCmBxxGw44j8qJ3r)\n",
        "![8.png](https://drive.google.com/uc?id=19_NCIqcjcGi8ZHjgB_x21wW4w3cpNAOC)\n",
        "![15.png](https://drive.google.com/uc?id=1M9K2VwD52uy-RoaY2U76gIRACVcs41Xj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9UcMOwSQNK5"
      },
      "outputs": [],
      "source": [
        "def evaluate(start, end, batch_size, data, graphical=False):\n",
        "    l = []\n",
        "    io = []\n",
        "    for i in tqdm(range(start, end, batch_size)):\n",
        "        batch_data = data[i:i+batch_size]\n",
        "        loss, giou = evaluate_batch_routine(bboxer, loss_fn, feature_extractor, batch_data, graphical=graphical)\n",
        "        l.append(loss)\n",
        "        io.append(giou)\n",
        "    print(f\"loss: {sum(l)/len(l)}, iou: {sum(io)/len(io)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1MM7f1iQQNK5"
      },
      "source": [
        "Run on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZwUNipAQNK5"
      },
      "outputs": [],
      "source": [
        "evaluate(0, len(test_data), 16, test_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajd0WxP3QNK5"
      },
      "source": [
        "Run on the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VETB9TygQNK5"
      },
      "outputs": [],
      "source": [
        "whole = train_data + validate_data + test_data\n",
        "evaluate(0, len(whole), 32, whole)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T_EbdfFAQNK5"
      },
      "source": [
        "## Conclusions and Future Work\n",
        "\n",
        "We noticed that the `HeatmapToBox` model is **highly dependant** on the quality of the heatmaps generated. If the heatmap is good, the training process is extremely faster and more effective.\n",
        "\n",
        "This means that extensively training the Spatial CLIP model on the dataset would result is extremely better performance.\n",
        "\n",
        "We didn't have the possibility to train the Spatial CLIP model on the dataset due to having finished the available hours of the Azure's server. This means that the regressor is extremely subjective to overfitting since we trained on very small raining data on our personal computers.\n",
        "\n",
        "Although the examples are drawn from the data the model is trained on, it can be seen that the model is able to **effectively represent** the data, suggesting that further regressor training togheter with the training of the Spatial CLIP could result in very good performance.\n",
        "\n",
        "The proposed model is interesting due to the following reasons:\n",
        "- It is a **single stage model** to perform phrase grounding\n",
        "- It enables the CLIP model to be used for phrase grounding\n",
        "- The modularity of the framework enabled the head to be swapped extremely easily. This means that any head capable of regressing a bounding box from an heatmap in the proposed form can be attached\n",
        "- It involves the possibility to finetune CLIP itself for the phrase grounding task\n",
        "\n",
        "The model was able to perfectly overfit small samples of the training data, demonstrating that its strucutre is able to actually represent the data samples in an efficient way.\n",
        "\n",
        "#### Problems\n",
        "\n",
        "We found that the model required a lot of time to be trained, we finished very fast our time available on the Azure server.\n",
        "\n",
        "Also the problem of overfitting was quite high in our model. The reasons may be reconducted to the fact that our head does not include transfer learning and must be trained from scratch. Also the size of the dataset we were able to train the model on played an important role.\n",
        "\n",
        "Techniques such as `Dropout` layers and `BarchNorm` layers have also been included into the model.\n",
        "\n",
        "#### Improvements\n",
        "\n",
        "The model could be improved in the following ways:\n",
        "- Extensive training on larger dataset, with better hardware resources\n",
        "- Tweaks to the shape and the hyperparameters of the `HeatmapToBox` head model\n",
        "- Finetuning the Spatial CLIP model itself in order to better adapt it to the target dataset\n",
        "\n",
        "Data augmentation has been tested. However, we believe that it may not be the most crucial point in fixing the overfitting problem as the RefCOCOg dataset includes different prompts for the same image and bounding box. This can be effectively already be seen as a text augmentation. From the point of view of the model, the same image with different prompts, must produce the same bounding box.\n",
        "\n",
        "\n",
        "The results showed that potentially this structure may serve as a good fundation for phrase grounding framework that are able to adapt pretrained larger models such as CLIP for a specific task such as phrase grounding."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FRmK6_ByQNK6"
      },
      "source": [
        "# Bibliography\n",
        "- [1] [Adapting CLIP For Phrase Localization Without Further Training](https://arxiv.org/abs/2204.03647)\n",
        "- [2] [A Fast and Accurate One-Stage Approach to Visual Grounding](https://arxiv.org/abs/1908.06354)\n",
        "- [3] [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580)\n",
        "- [4] [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
